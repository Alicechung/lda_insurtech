AMAX To Exhibit Full Line Of Scalable Deep Learning And AI Platforms At GTC 2018

27 Mar 2018

AMAX is a trusted GPU solutions provider for the top AI and enterprise companies in the world. As an NVIDIA Elite Partner, AMAX offers the most comprehensive line of GPU-integrated solutions optimized for Deep Learning at any scale— from high-performance workstations for development to large-scale production-ready GPU clusters complete with integrated high-density compute, storage, networking and system health/power management features. AMAX is able to custom configure its solutions based on each customer’s compute requirements, stage of development or deployment and performance goals, scaling as those needs grow and evolve. AMAX’s Deep Learning platforms feature unmatched performance-per-dollar and performance-per-watt level.
AMAX’s platforms featured at GTC 2018 include:
DL-E48X Configurable PCIe Root Complex Deep Learning Server – The DL-E48X is the very first system on the market that features re-configurable dual root complex and single root complex PCIe architectures, allowing for hardware optimization on the fly for AI and DL training, inference, HPC compute, rendering and virtualization applications. Utilizing Intel’s latest Xeon Scalable Processors (Skylake), the system provides a 56 percent increase in memory bandwidth and a 54 percent bandwidth increase for communication between CPUs (UPI) compared to the previous Intel Processor generation. The DL-E48X can host up to 8x NVIDIA® Tesla® V100 and P100 GPUs and supports up to 4x NVMe drives and up to 2x HDR InfiniBand (100GB) network adapters, making the DL-E48X the ideal system to handle any workload where GPU acceleration is required.
ServMax™ HGX-1 Hyperscale AI Cloud Solution – Powered by NVIDIA Tesla GPUs and NVLink™ high-speed interconnect technology, the HGX-1 is purpose-built for AI/HPC cloud computing, providing revolutionary performance, configurability and future-proofing. Hosting 8x Tesla V100 SXM2 GPUs in a 4U chassis, the HGX-1 features 5,120 Tensor Cores and 40,960 CUDA cores for 1 PFLOP of Tensor Core operations and 126 TFLOPS of SP performance. With its modular design, HGX-1 is suited for deployment in existing data center racks across the globe, offering hyperscale data centers a quick, simple path to be ready for AI.
[SMART]RACK AI –  The [SMART]Rack AI is a turnkey Machine Learning cluster designed for optimal manageability and performance, featuring 96x NVIDIA Tesla V100s, P100s or P40s for up to 1.34 PFLOPs per rack. Delivered plug-and-play and fully loaded, the solution features All-Flash storage for an ultra-fast in-rack data repository, dual 25G high-speed networking or optional EDR InfiniBand, [SMART]DC Data Center Manager and an optional in-rack battery for graceful shutdown in power-loss scenarios. [SMART]Rack AI is the perfect platform for on-premise AI clouds and DL-as-a-service or to drop into any data center environment for the highest performance in training and inference at scale.
[SMART]DC GPU-Optimized Data Center Manager – The [SMART]DC Data Center Manager is the premier out-of-band DCIM solution for the modern-day heterogeneous data center. Built on a robust and field-proven software platform, [SMART]DC seamlessly manages GPU hardware alongside compute and storage, using a single pane of glass. For power-dense Deep Learning deployments where real-time system health is critical to ensure uninterrupted operation or for deployments in data rooms without data center infrastructure, [SMART]DC not only protects your investment by providing insights for real-time temperature fluctuation, power consumption, capacity planning and server under- and over-utilization, but features fully automated policy-based server orchestration to maximize server health and data center power and efficiency management.
DL-E400 Deep Learning Workstation – Powered by 4x NVIDIA Titan V/1080 Ti GPUs, the DL-E400 is an end-to-end “DL-in-a-Box” solution with everything needed to fast-track AI development. Coming from a comprehensive line of plug-and-play DL appliances, the DL-E400 eliminates hardware compatibility and setup overhead to significantly elevate time-to-productivity.
Julia Shih
For AI companies interested in deploying their software as a turnkey on-premise appliance, AMAX offers OEM services to design, validate and mass produce GPU-integrated workstation or server appliances featuring unique custom-branding options. AMAX’s field-proven NPI (New Product Introduction) program can provide a custom-branded prototype in as little as 30 days, enabling AI startups to quickly bring new products to market for a competitive advantage.
“AMAX has a proven track record for supporting AI initiatives at any and every scale,” said Julia Shih, VP of business development, AMAX. “With AI being one of the most exciting evolutions of our lifetimes, we are committed to working with leading companies to define a clear compute strategy, then execute using the best-performing and highest-quality GPU solutions on the market to support their success.”


