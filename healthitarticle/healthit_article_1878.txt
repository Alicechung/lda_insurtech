Open source mentality brings new data visualization techniques

August 27, 2014

By Jennifer Bresnick
August 27, 2014 - While healthcare organizations are still striving to understand the scope and usefulness of big data for clinical analytics and financial insights, some data scientists and software developers are focusing on the next step: how to turn unimaginable quantities of information into clear and distinct visual representations that allow for easy decision-making when and where it counts.  Far from being a simple graphic design issue, data visualization is critical for bringing meaning and actionable insights to life, and can spell the difference between a useful, widely-adopted product and a confusing, frustrating implementation failure.
With a background in mathematics and 14 years of consulting expertise in the analytics and data mining space, Steven Escaravage, Principal in Booz Allen’s Strategic Innovation Group, has plenty of experience with developing and using data analytics interfaces.  Escaravage sat down with HealthITAnalytics to discuss the importance of good data visualization in the healthcare industry and how open source development is changing the way better tools come to market.
To read the second part of this interview, please click here.
How has data science changed as a field of study during your time in the industry?
In the discipline of data science, there’s real innovation and invention in the methods and techniques that are applied against the data. Over the past decade, we’ve seen that many of the information systems, or informatics systems, are just more robust in terms of their ability to present and collect data.  We used to have this mentality of “I can only work with data or pull data into my architecture if it fits the code; if it fits the schema by which that architecture was created,” and there were techniques where you could add new variables, add new tables, add new data marks to your data warehouse system.
READ MORE: Smart Big Data is Key to Population Health, Value-Based Care
What we’ve seen over the last decade is just so much more flexibility into getting data into the systems.  The challenge with that, though, is that once you get the data in the systems, you need to deliver all these amazing algorithm outputs and all these unknown patterns to the end user in a manner that is intuitive, clear, and efficient.  I think this area of data visualization, data translation, and translational science applies to all the key protocols within the health field.
Why is it so important to provide clear and concise visualizations of data?
We continue to see literature published around new machine learning methods and really intense, complicated approaches to mining all this data.  But at the end of the day, we’ve got to deliver that finding, that insight, to the decision maker in a way that is intuitive.  We need to make sure they don’t get “data overload.”
I think where we’re going to see continued investment and continued innovation is figuring out that piece of it.  How do you flag to a user that there’s some event of interest taking place in a timed series of events?  How do you show a pattern or a relationship, an association between two events?  How do you show it so that it jumps off the screen to a provider during their workflow, and really inserts itself in their brain? They need to know immediately that they’ve got to take a deeper look at this, and that this event is different or more important than the last hundreds or thousands of triggers or flags that have been thrown up.
I think that’s where you’re going to see this intersection of data scientists, graphic artists, and social scientists coming together, and I think you’re going to see some really beautiful inventions related to how to project information to an end user that allows them to make some decisions.
READ MORE: ONC Updates EHR Usability, Health IT Patient Safety Guides
What are some of the key changes to architecture development that has allowed the science of data visualization to flourish?
The past few years have been really exciting from our standpoint.  There have been a lot of very powerful technologies that have been released.  In the past, you’d have to purchase a license for a tool that allowed you to work with data that was of a certain scale or a certain complexity, to be able to experiment with dashboards and visualizations.   I would hire people ten years ago to put them on a research project or a client project, and the first thing I would have to do is go buy a bunch of very expensive licenses that they would use.
Now, when I hire a PhD out of an academic program or research group, they come with their preference around open source tools and technologies.  They’re coming with their view of which Java libraries to use, and they’re coming with this portfolio of their work, almost like you would if you were an artist, containing these visualizations that they’ve experimented with.  It’s so much easier now for them to just jump in quickly because there are all these open source tools out there that can handle big data.
They can handle data at scale, and they have very favorable licenses in terms of how you can use them and how you can apply them.  What’s great is that the community, I think, is really adopting an open source mentality.  When you have a new invention, there’s a responsibility on the part of the user to submit that back to the open source community so they can use it, and we’re certainly a big proponent of, and big contributor to, open source materials.
Does the proliferation of open source product fragment the market too much for providers looking to make good choices, or is a positive development for those seeking customized and flexible tool sets?
READ MORE: Artificial Intelligence in Healthcare Market to See 40% CAGR Surge
I see it as a good thing in the long run because it removes some of the time and the cost in the initial development phase.  When you have more product suites available, it will certainly fragment a market, but I think that the value that’s obtained in the early states by being able to quickly get in and prototype and pilot things in a very inexpensive way outweighs the challenges of having multiple different platforms that might need to be integrated at the end of the day.
I would also say that with most of the tools now, be it proprietary commercial software or open source software, we’ve seen a greater willingness and a greater capability to integrate and become interoperable.  Most of your commercial vendors are realizing the power in these toolkits, so they’ll include APIs into computation programs or interfaces.   Even five years ago, there wasn’t as much incentive to allow you to plug in to an open source library or toolkit.
Whenever you have a limited set of established technologies that are dominating a market, it becomes more difficult to be really agile and to change that product very rapidly to respond to new requests.  Startups that come online can just be more flexible and more rapid, and so their innovation timeline shrinks.   I think that cycle of innovation that you see in the technology world – I would assume we’re going to continue to see that with new data visualization capabilities, new data ingest capabilities, and certainly for some of the new data science capabilities that are coming out in terms of logarithms, methods, and techniques.
Tagged Analytics InfrastructureBig DataHealthcare Data Analytics


